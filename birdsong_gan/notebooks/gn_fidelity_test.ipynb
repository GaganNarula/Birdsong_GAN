{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ultimate-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "australian-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "modified-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import joblib\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "entertaining-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "frozen-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "indian-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import GaussianHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acute-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm.hmm_utils import tempered_sampling\n",
    "from utils.utils import load_netG, overlap_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "maritime-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruction_error.fidelity import evaluate_generative_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "convinced-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import bird_dataset_single_hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "major-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = dict(modelpath = '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/',\n",
    "           datapath = '/home/songbird/datapartition/all_birds.hdf',\n",
    "           birdname = 'p3r16',\n",
    "            cuda = 'True',\n",
    "            alpha = 0.95,\n",
    "            beta = 0.96,\n",
    "            nu = 1.,\n",
    "            nrnn = 100,\n",
    "            nrnnlayers = 1,\n",
    "            dropout = 0.0,\n",
    "            batch_size = 64,\n",
    "            nepochs = 30,\n",
    "            lr = 1e-3,\n",
    "            l2 = 0.0,\n",
    "            beta1 = 0.5,\n",
    "            log_every = 100,\n",
    "            checkpoint_models = False,\n",
    "            \n",
    "            nembed = 3,\n",
    "            nlin = 50,\n",
    "            kneighbors = 10,\n",
    "            max_length=100\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which days were trained for this bird and model type\n",
    "day_folders = sorted(glob(join(opts['modelpath'], 'day*')))\n",
    "\n",
    "# make bird dataset\n",
    "dataset = bird_dataset_single_hdf(opts['datapath'], opts['birdname'])\n",
    "\n",
    "# model opts\n",
    "with open(join(opts['modelpath'], 'opts.json'), 'rb') as f:\n",
    "    model_opts = json.load(f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "handed-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_9'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_folders[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the day index from the name\n",
    "dayname = os.path.split(day)[-1]\n",
    "dayidx = int(dayname.split('_')[-1])\n",
    "print('..... working on day %s .....'%(dayname))           \n",
    "\n",
    "# get real data for this day \n",
    "X = dataset.get(dayidx, nsamps=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cloudy-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dynamic-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find which netG to use\n",
    "netGfilepath = sorted(glob(join(day, 'netG_*')))[-1]\n",
    "\n",
    "netG = load_netG(netGfilepath, model_opts['nz'], model_opts['ngf'], model_opts['nc'],\n",
    "                opts['cuda'], resnet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cardiac-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reconstruction_error.fidelity import evaluate_generative_model, learn_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_models = glob(join(day, 'hmm_*'))\n",
    "\n",
    "#for k in range(len(hmm_models)):\n",
    "k = 0\n",
    "\n",
    "# load hmm model\n",
    "hmm = joblib.load(join(day, hmm_models[k], 'model_day_'+str(dayidx)+'.pkl'))\n",
    "hmm = hmm['model']\n",
    "\n",
    "# first train embedding on real data\n",
    "if embedder is None:\n",
    "    embedder = learn_embedding(X, opts)\n",
    "    embedder.eval()\n",
    "\n",
    "metrics = evaluate_generative_model(netG, hmm, X, opts, embedder)\n",
    "\n",
    "avg_precision = metrics[0].cpu().numpy().mean()\n",
    "avg_recall =  metrics[1].cpu().numpy().mean()\n",
    "avg_authenticity =  metrics[2].cpu().numpy().mean()\n",
    "\n",
    "print(f\"..... day {dayname}, hmm states {hmm.transmat_.shape[0]} avg precision {avg_precision} .....\")\n",
    "print(f\"..... day {dayname}, hmm states {hmm.transmat_.shape[0]} avg recall {avg_recall} .....\")\n",
    "print(f\"..... day {dayname}, hmm states {hmm.transmat_.shape[0]} avg authenticity {avg_authenticity} .....\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "reasonable-essay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_10',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_12',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_15',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_20',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_25',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_30',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_40',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_5',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_50',\n",
       " '/home/songbird/datapartition/mdgan_output/daily_gan/p3r16_nz12/day_10/hmm_hiddensize_60']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_models = sorted(glob(join(day, 'hmm_*')))\n",
    "hmm_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "every-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "N = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "unique-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... generating fake samples .....\n",
      "..... embedding fakes and real samples .....\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    print('..... generating fake samples .....')\n",
    "    timesteps = [seq.shape[1]//16 for seq in data]\n",
    "    sample_seqs = generate_samples(netG, hmm, N, 1., timesteps, opts['cuda'])\n",
    "\n",
    "    # embed all\n",
    "    print('..... embedding fakes and real samples .....')\n",
    "    fake_pts = embed_list_of_sequences(embedder, sample_seqs, cuda=opts['cuda'])\n",
    "    real_pts = embed_list_of_sequences(embedder, data, cuda=opts['cuda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "plastic-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... learning nearest neighbors for all reals .....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nearest neighbor\n",
    "print('..... learning nearest neighbors for all reals .....')\n",
    "nearest_neighbors = knn(real_pts, opts['kneighbors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "extraordinary-roots",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..... alpha radius for real ball is 0.0152 .....\n",
      "..... computing precision for all fake samples .....\n",
      "..... beta radius for fake ball is 0.0185 .....\n",
      "..... computing recall for all fake samples .....\n",
      "..... computing authenticity for all fake samples .....\n"
     ]
    }
   ],
   "source": [
    " # generate fake sample sequences\n",
    "with torch.no_grad():\n",
    "\n",
    "    # radius of the real Ball \n",
    "    center_r = torch.zeros(opts['nembed'])\n",
    "    if opts['cuda']:\n",
    "        center_r = center_r.cuda()\n",
    "\n",
    "    r_alpha = compute_quantile_radius(real_pts, center_r, opts['alpha'])\n",
    "    print('..... alpha radius for real ball is %.4f .....'%(float(r_alpha)))\n",
    "\n",
    "    # sample wise metrics\n",
    "    # precision\n",
    "    print('..... computing precision for all fake samples .....')\n",
    "    P = np.array([precision_classifier(fake_pts[i], center_r, r_alpha) for i in range(N)])\n",
    "\n",
    "    # recall\n",
    "    # make center of fake Ball the average fake data\n",
    "    c_g = fake_pts.mean(dim=0)\n",
    "    # get ball B radius for ball of generated samples\n",
    "    r_beta = compute_quantile_radius(fake_pts, c_g, opts['beta'])\n",
    "    print('..... beta radius for fake ball is %.4f .....'%(r_beta))\n",
    "\n",
    "    print('..... computing recall for all fake samples .....')\n",
    "    R = np.array([recall_classifier(y, fake_pts, nearest_neighbors, r_beta, opts['kneighbors'],\n",
    "                      beta=opts['beta']) for y in real_pts])\n",
    "\n",
    "    # authenticity \n",
    "    print('..... computing authenticity for all fake samples .....')\n",
    "    A = np.array([authenticity_classifier(y, real_pts) for y in fake_pts]).mean()\n",
    "    \n",
    "    D, C = density_and_coverage(fake_pts, real_pts, opts['kneighbors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "coupled-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6516634050880626\n"
     ]
    }
   ],
   "source": [
    "print(P.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "editorial-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(R.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "straight-singles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9980430528375733\n"
     ]
    }
   ],
   "source": [
    "print(A.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "amino-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "female-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "def one_class_SVM_loss(embedded_x, center, radius, nu):\n",
    "    r\"\"\"Compute One class SVM loss for data points x.\n",
    "    ..math:\n",
    "        L = \\sum(l_i)\n",
    "        l_i = r^2 + (1/nu) * max(0, ||x_i - center||^2 - r^2)\n",
    "    \"\"\"\n",
    "    # for each sample compute L2 distance to center\n",
    "    n = embedded_x.shape[0] # num samples\n",
    "    l1 = torch.pow(embedded_x - center, 2).sum(dim=-1)\n",
    "    l2 = nn.functional.relu(l1 - radius**2)\n",
    "    \n",
    "    return (radius**2 + (1/nu)*l2).sum()\n",
    "\n",
    "\n",
    "pdist = nn.functional.pairwise_distance\n",
    "\n",
    "\n",
    "def compute_quantile_radius(x, center, alpha=0.95):\n",
    "    \"\"\"Given a set of data points in embedded / data space and a center\n",
    "        returns: \n",
    "    \"\"\" \n",
    "    # pairwise distances between each point x and center \n",
    "    D = pdist(x, center)\n",
    "    \n",
    "    return torch.quantile(D, alpha)\n",
    "    \n",
    "    \n",
    "def precision_classifier(fake_x, center, r_alpha):\n",
    "    \"\"\"Precision classifier tests if the fake sample 'fake_x'\n",
    "        outside of the alpha ball around reals B(center_r, r_alpha)\n",
    "    \"\"\"\n",
    "    pred = pdist(fake_x.view(1,-1), center.view(1,-1)) <= r_alpha\n",
    "    return float(pred.detach())\n",
    "\n",
    "def knn(x, K=10):\n",
    "    \"\"\"Make a sklearn.neighbors object for datapoints in tensor x\n",
    "        x has shape (N, D), N = num points\n",
    "    \"\"\"\n",
    "    x = make_numpy(x)\n",
    "    \n",
    "    return NearestNeighbors(n_neighbors=K, algorithm='ball_tree').fit(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "def recall_classifier(real_x, fake_x_pts, nearest_neighbors, r_beta=None, K=10, beta=0.95):\n",
    "    \"\"\"\n",
    "    1. Make a Ball(center_g, radius_beta). To do so, use the fake_x_pts \n",
    "        to get the quantile radius, eliminate fake pts outside the quantile radius.\n",
    "    2. Find the nearest neighbor distance of real_x to all other reals\n",
    "    3. Find the nearest fake to real_x, lying in the ball B(c_g, r_beta)\n",
    "    4. Return : Is distance between nearest_fake and real_x <= NND ? \n",
    "    \n",
    "    \"\"\"\n",
    "    # make center the average fake data\n",
    "    c_g = fake_x_pts.mean(dim=0).view(1,-1)\n",
    "    \n",
    "    if r_beta is None:\n",
    "        # get ball B radius\n",
    "        r_beta = compute_quantile_radius(fake_x_pts, c_g, beta)\n",
    "        \n",
    "    # which pts are in the ball ? \n",
    "    in_ball = pdist(fake_x_pts, c_g) <= r_beta\n",
    "    \n",
    "    # find the K nearest neighbors of real_x in all other reals\n",
    "    rx = make_numpy(real_x)\n",
    "    dists, _ = nearest_neighbors.kneighbors(rx.reshape(1,-1), K)\n",
    "    NND = dists.squeeze()[-1] # Kth nearest neighbor\n",
    "    \n",
    "    # find nearest pt to real_x in ball B(c_g, r_beta)\n",
    "    D = pdist(fake_x_pts[in_ball], real_x)\n",
    "    nearest_fake = torch.argmin(D)\n",
    "    \n",
    "    # is nearest fake in Ball(real_x, NND) ? \n",
    "    pred = pdist(fake_x_pts[nearest_fake].view(1,-1), real_x.view(1,-1)) <= NND\n",
    "    return float(pred.detach())\n",
    "    \n",
    "\n",
    "def authenticity_classifier(fake_x, real_x_pts):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # find distance between fake pt and all reals\n",
    "    D = pdist(fake_x, real_x_pts)\n",
    "    # find nearest real to fake\n",
    "    i = torch.argmin(D) # i* in paper\n",
    "    d_g_j = torch.min(D) # d_{g,j} in paper\n",
    "    \n",
    "    real_x_pts_wo = torch.cat([real_x_pts[:i],real_x_pts[i+1:]],dim=0)\n",
    "    \n",
    "    d_r_i = pdist(real_x_pts[i], real_x_pts_wo).min() # smallest distance to rest\n",
    "    \n",
    "    # output of authenticity classifier is A_j=1 (authentic) if the \n",
    "    # distance d_r_i < d_g_j\n",
    "    pred = d_r_i < d_g_j\n",
    "    return float(pred.detach())\n",
    "        \n",
    "\n",
    "def density_and_coverage(fake_x, real_x_pts, K=10):\n",
    "    \"\"\"From Naeem et al 2020.\n",
    "        Density is defined as (1/kM)*(1/n) \\sum_j \\sum_i (Is fake_j  in ball B(x_i, NND_k(x_i))\n",
    "    \"\"\"\n",
    "    nearest_neighbors = knn(real_x_pts, K)\n",
    "    \n",
    "    D = []\n",
    "    C = np.zeros(len(real_x_pts))\n",
    "    M = fake_x.shape[0]\n",
    "    for i in range(real_x_pts.shape[1]):\n",
    "        \n",
    "        for j in range(M):\n",
    "            \n",
    "            # is Y_j in Ball around X_i ?\n",
    "            # what is the Ball around X_i\n",
    "            rx = make_numpy(real_x_pts[i])\n",
    "            dists, _ = nearest_neighbors.kneighbors(rx.reshape(1,-1), n_neighbors=K,\n",
    "                                                         return_distance=True)\n",
    "            # find the Kth neighbor distance\n",
    "            NND = dists.squeeze()[-1]\n",
    "            \n",
    "            pred = pdist(fake_x[j].view(1,-1), real_x_pts[i].view(1,-1)) <= NND\n",
    "            D.append(float(pred.detach()))\n",
    "            \n",
    "            # for coverage\n",
    "            if D[-1] and C[i] == 0.:\n",
    "                C[i] = 1.\n",
    "            \n",
    "    D = np.sum(D)\n",
    "    return (1/K)*(1/M)*D, C.mean()\n",
    "            \n",
    "            \n",
    "    \n",
    "def learn_embedding(data, opts):\n",
    "    print('..... making tensor dataset and dataloader .....')\n",
    "    traindataset = make_dataset(data, opts['max_length'])\n",
    "    traindataloader = DataLoader(traindataset, batch_size=opts['batch_size'], shuffle=True, drop_last=True)\n",
    "    \n",
    "    print('..... training embedding .....')\n",
    "    embedder = EmbeddingNet(data[0].shape[0], opts['nembed'], nrnn=opts['nrnn'], nrnnlayers=opts['nrnnlayers'],\n",
    "                       dropout=opts['dropout'], nlin=opts['nlin'])\n",
    "    if opts['cuda']:\n",
    "        embedder = embedder.cuda()\n",
    "\n",
    "    return train(embedder, traindataloader, opts)\n",
    "\n",
    "\n",
    "def embed_list_of_sequences(embedder, seqs, cuda=True):\n",
    "    Xtilde = []\n",
    "    for x in seqs:\n",
    "        x = torch.from_numpy(x).to(torch.float32)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        Xtilde.append(embedder(x.view(1, x.shape[0], x.shape[1])))\n",
    "    return torch.cat(Xtilde, dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "certain-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(netG, hmm, nsamples=1, invtemp=1., timesteps=[], cuda=True):\n",
    "    \"\"\"Generate samples from trained netG and hmm\"\"\"\n",
    "    seqs = [tempered_sampling(hmm, invtemp, timesteps=timesteps[i], \n",
    "                                sample_obs=True, start_state_max=True, \n",
    "                                 sample_var = 0.)[0] for i in range(nsamples)]\n",
    "    # decode with netG\n",
    "    seqs_out = [None for _ in range(len(seqs))]\n",
    "    for i in range(len(seqs)):\n",
    "        seqs_out[i] = overlap_decode(seqs[i], netG,  noverlap = 0,\n",
    "                                          cuda = cuda, get_audio = False)[0]\n",
    "    \n",
    "    return seqs_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-offense",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
